<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Power Outage Analysis | power_outage_analysis</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Power Outage Analysis" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="power_outage_analysis" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Power Outage Analysis" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"Power Outage Analysis","name":"power_outage_analysis","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=9321e1960183a714ed9b802b1a1f2e7f3fcba9c4">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Power Outage Analysis</h1>
      <h2 class="project-tagline"></h2>
      
        <a href="https://github.com/vip11017/power_outage_analysis" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="power-outage-analysis">Power Outage Analysis</h1>

<h2 id="introduction">Introduction</h2>
<p>In this project I studied the relationships between the cause of a power outage with many other features such as climate, state, electricity price, etc. The dataset I used for this was provided by Purudue University Laboratory for Advancing Sustainable Critical Infrastructure, titled Major Power Outage Risks in the U.S. This dataset has various data regarding certain characteristics of power otuages, such as location, electricity consumoption, and population. The dataset has 1534 observations and 55 features. With this dataset I based my project around the question: “What characteristics are asociated with each category of cause?”. This website will provide you insights on some patterns and characteristiscs of major power outages. You will also be able to learn more about the many aspects of a power outage from location to after-effect. Some of the features in the dataset that will provide you this information are:</p>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">U.S._State</code>: Represents the state in continental U.S. where the outage occured <br />
<code class="language-plaintext highlighter-rouge">NERC.REGION</code>: The North American Electricity Reliability Corportation regions involved in the outage event <br />
<code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code>: U.S. Climate Regions based by National Centers for Environmental Information <br />
<code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY</code>: Represents the climate episodes corresponding to years. <br />
<code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code>: Categories of events causing the power outage <br />
<code class="language-plaintext highlighter-rouge">OUTAGE.DURATION</code>: Duration of Outage <br />
<code class="language-plaintext highlighter-rouge">DEMAND.LOSS.MW</code>: Amount of peak demand lost during an outage event <br />
<code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code>: Number of customers affected by power outage event</p>
</blockquote>

<h2 id="data-cleaning-and-exploratory-data-analysis">Data Cleaning and Exploratory Data Analysis</h2>
<h4 id="data-cleaning">Data Cleaning</h4>

<p>The column <code class="language-plaintext highlighter-rouge">OUTAGE.START.DATE</code> and <code class="language-plaintext highlighter-rouge">OUTAGE.START.TIME</code> are both <code class="language-plaintext highlighter-rouge">pd.timestamp()</code> values, so I combined them to mame a new <code class="language-plaintext highlighter-rouge">OUTAGE.START</code> column containing the start date and time for the outage</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.START.DATE'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.START.DATE'</span><span class="p">].</span><span class="n">dt</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.START.TIME'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.START.TIME'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.START'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.START.DATE'</span><span class="p">]</span> <span class="o">+</span> <span class="s">" "</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.START.TIME'</span><span class="p">])</span>
</code></pre></div></div>

<p>The column <code class="language-plaintext highlighter-rouge">OUTAGE.RESTORATION.DATE</code> and <code class="language-plaintext highlighter-rouge">OUTAGE.RESTORATION.TIME</code> are also a <code class="language-plaintext highlighter-rouge">pd.timestamp()</code> type, so I did the same thing</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.RESTORATION.DATE'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.RESTORATION.DATE'</span><span class="p">].</span><span class="n">dt</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.RESTORATION.TIME'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.RESTORATION.TIME'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.RESTORATION'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.RESTORATION.DATE'</span><span class="p">]</span> <span class="o">+</span> <span class="s">" "</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s">'OUTAGE.RESTORATION.TIME'</span><span class="p">])</span>
</code></pre></div></div>

<p>The columns <code class="language-plaintext highlighter-rouge">RES.PRICE</code>, <code class="language-plaintext highlighter-rouge">COM.PRICE</code>, <code class="language-plaintext highlighter-rouge">IND.PRICE</code>, <code class="language-plaintext highlighter-rouge">TOTAL.PRICE</code> are currently in the unit of cents/kilowatt-hour. So to makle it on the same unit as some of the other columns measures in dollars, I converted this column to the units of dollars/kilowatt-hour</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">price_col</span> <span class="o">=</span> <span class="p">[</span><span class="s">'RES.PRICE'</span><span class="p">,</span> <span class="s">'COM.PRICE'</span><span class="p">,</span> <span class="s">'IND.PRICE'</span><span class="p">,</span> <span class="s">'TOTAL.PRICE'</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="n">price_col</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">100</span>
</code></pre></div></div>

<p>The columns <code class="language-plaintext highlighter-rouge">RES.SALES</code>, <code class="language-plaintext highlighter-rouge">COM.SALES</code>, <code class="language-plaintext highlighter-rouge">IND.SALES</code>, <code class="language-plaintext highlighter-rouge">TOTAL.SALES</code>, and <code class="language-plaintext highlighter-rouge">DEMAND.LOSS.MW</code> were all measures in the unit of megawatt. So, to make it on the same measurement as the other features, I transformed it to kilowatts-hour</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sales_col</span> <span class="o">=</span> <span class="p">[</span><span class="s">'RES.SALES'</span><span class="p">,</span> <span class="s">'COM.SALES'</span><span class="p">,</span> <span class="s">'IND.SALES'</span><span class="p">,</span> <span class="s">'TOTAL.SALES'</span><span class="p">,</span> <span class="s">'DEMAND.LOSS.MW'</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="n">sales_col</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">1000</span>
</code></pre></div></div>

<p>The columns that are stored as a percentage(%), so to ease future mathematical operations, I converted them all into decimal format</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">perc_col</span> <span class="o">=</span> <span class="p">[</span><span class="s">'RES.PERCEN'</span><span class="p">,</span> <span class="s">'COM.PERCEN'</span><span class="p">,</span> <span class="s">'IND.PERCEN'</span><span class="p">,</span> <span class="s">'RES.CUST.PCT'</span><span class="p">,</span> <span class="s">'COM.CUST.PCT'</span><span class="p">,</span> <span class="s">'IND.CUST.PCT'</span><span class="p">,</span> <span class="s">'PC.REALGSP.CHANGE'</span><span class="p">,</span> <span class="s">'UTIL.CONTRI'</span><span class="p">,</span> <span class="s">'PI.UTIL.OFUSA'</span><span class="p">,</span> <span class="s">'POPPCT_URBAN'</span><span class="p">,</span> <span class="s">'POPPCT_UC'</span><span class="p">,</span> <span class="s">'AREAPCT_URBAN'</span><span class="p">,</span> <span class="s">'AREAPCT_UC'</span><span class="p">,</span> <span class="s">'PCT_LAND'</span><span class="p">,</span> <span class="s">'PCT_WATER_TOT'</span><span class="p">,</span> <span class="s">'PCT_WATER_INLAND'</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="n">perc_col</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">100</span>
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right">YEAR</th>
      <th style="text-align: right">MONTH</th>
      <th style="text-align: left">U.S._STATE</th>
      <th style="text-align: left">POSTAL.CODE</th>
      <th style="text-align: left">NERC.REGION</th>
      <th style="text-align: left">CLIMATE.REGION</th>
      <th style="text-align: right">ANOMALY.LEVEL</th>
      <th style="text-align: left">CLIMATE.CATEGORY</th>
      <th style="text-align: left">OUTAGE.START.DATE</th>
      <th style="text-align: left">OUTAGE.START.TIME</th>
      <th style="text-align: left">OUTAGE.RESTORATION.DATE</th>
      <th style="text-align: left">OUTAGE.RESTORATION.TIME</th>
      <th style="text-align: left">CAUSE.CATEGORY</th>
      <th style="text-align: left">CAUSE.CATEGORY.DETAIL</th>
      <th style="text-align: right">HURRICANE.NAMES</th>
      <th style="text-align: right">OUTAGE.DURATION</th>
      <th style="text-align: right">DEMAND.LOSS.MW</th>
      <th style="text-align: right">CUSTOMERS.AFFECTED</th>
      <th style="text-align: right">RES.PRICE</th>
      <th style="text-align: right">COM.PRICE</th>
      <th style="text-align: right">IND.PRICE</th>
      <th style="text-align: right">TOTAL.PRICE</th>
      <th style="text-align: right">RES.SALES</th>
      <th style="text-align: right">COM.SALES</th>
      <th style="text-align: right">IND.SALES</th>
      <th style="text-align: right">TOTAL.SALES</th>
      <th style="text-align: right">RES.PERCEN</th>
      <th style="text-align: right">COM.PERCEN</th>
      <th style="text-align: right">IND.PERCEN</th>
      <th style="text-align: right">RES.CUSTOMERS</th>
      <th style="text-align: right">COM.CUSTOMERS</th>
      <th style="text-align: right">IND.CUSTOMERS</th>
      <th style="text-align: right">TOTAL.CUSTOMERS</th>
      <th style="text-align: right">RES.CUST.PCT</th>
      <th style="text-align: right">COM.CUST.PCT</th>
      <th style="text-align: right">IND.CUST.PCT</th>
      <th style="text-align: right">PC.REALGSP.STATE</th>
      <th style="text-align: right">PC.REALGSP.USA</th>
      <th style="text-align: right">PC.REALGSP.REL</th>
      <th style="text-align: right">PC.REALGSP.CHANGE</th>
      <th style="text-align: right">UTIL.REALGSP</th>
      <th style="text-align: right">TOTAL.REALGSP</th>
      <th style="text-align: right">UTIL.CONTRI</th>
      <th style="text-align: right">PI.UTIL.OFUSA</th>
      <th style="text-align: right">POPULATION</th>
      <th style="text-align: right">POPPCT_URBAN</th>
      <th style="text-align: right">POPPCT_UC</th>
      <th style="text-align: right">POPDEN_URBAN</th>
      <th style="text-align: right">POPDEN_UC</th>
      <th style="text-align: right">POPDEN_RURAL</th>
      <th style="text-align: right">AREAPCT_URBAN</th>
      <th style="text-align: right">AREAPCT_UC</th>
      <th style="text-align: right">PCT_LAND</th>
      <th style="text-align: right">PCT_WATER_TOT</th>
      <th style="text-align: right">PCT_WATER_INLAND</th>
      <th style="text-align: left">OUTAGE.START</th>
      <th style="text-align: left">OUTAGE.RESTORATION</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">2011</td>
      <td style="text-align: right">7</td>
      <td style="text-align: left">Minnesota</td>
      <td style="text-align: left">MN</td>
      <td style="text-align: left">MRO</td>
      <td style="text-align: left">East North Central</td>
      <td style="text-align: right">-0.3</td>
      <td style="text-align: left">normal</td>
      <td style="text-align: left">2011-07-01</td>
      <td style="text-align: left">17:00:00</td>
      <td style="text-align: left">2011-07-03</td>
      <td style="text-align: left">20:00:00</td>
      <td style="text-align: left">severe weather</td>
      <td style="text-align: left">nan</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">3060</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">70000</td>
      <td style="text-align: right">0.116</td>
      <td style="text-align: right">0.0918</td>
      <td style="text-align: right">0.0681</td>
      <td style="text-align: right">0.0928</td>
      <td style="text-align: right">2.33292e+09</td>
      <td style="text-align: right">2.11477e+09</td>
      <td style="text-align: right">2.11329e+09</td>
      <td style="text-align: right">6.56252e+09</td>
      <td style="text-align: right">0.355491</td>
      <td style="text-align: right">0.32225</td>
      <td style="text-align: right">0.322024</td>
      <td style="text-align: right">2308736</td>
      <td style="text-align: right">276286</td>
      <td style="text-align: right">10673</td>
      <td style="text-align: right">2595696</td>
      <td style="text-align: right">0.889448</td>
      <td style="text-align: right">0.10644</td>
      <td style="text-align: right">0.00411181</td>
      <td style="text-align: right">51268</td>
      <td style="text-align: right">47586</td>
      <td style="text-align: right">1.07738</td>
      <td style="text-align: right">0.016</td>
      <td style="text-align: right">4802</td>
      <td style="text-align: right">274182</td>
      <td style="text-align: right">0.0175139</td>
      <td style="text-align: right">0.022</td>
      <td style="text-align: right">5348119</td>
      <td style="text-align: right">0.7327</td>
      <td style="text-align: right">0.1528</td>
      <td style="text-align: right">2279</td>
      <td style="text-align: right">1700.5</td>
      <td style="text-align: right">18.2</td>
      <td style="text-align: right">0.0214</td>
      <td style="text-align: right">0.006</td>
      <td style="text-align: right">0.915927</td>
      <td style="text-align: right">0.0840733</td>
      <td style="text-align: right">0.0547874</td>
      <td style="text-align: left">2011-07-01 17:00:00</td>
      <td style="text-align: left">2011-07-03 20:00:00</td>
    </tr>
    <tr>
      <td style="text-align: right">2014</td>
      <td style="text-align: right">5</td>
      <td style="text-align: left">Minnesota</td>
      <td style="text-align: left">MN</td>
      <td style="text-align: left">MRO</td>
      <td style="text-align: left">East North Central</td>
      <td style="text-align: right">-0.1</td>
      <td style="text-align: left">normal</td>
      <td style="text-align: left">2014-05-11</td>
      <td style="text-align: left">18:38:00</td>
      <td style="text-align: left">2014-05-11</td>
      <td style="text-align: left">18:39:00</td>
      <td style="text-align: left">intentional attack</td>
      <td style="text-align: left">vandalism</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">0.1212</td>
      <td style="text-align: right">0.0971</td>
      <td style="text-align: right">0.0649</td>
      <td style="text-align: right">0.0928</td>
      <td style="text-align: right">1.58699e+09</td>
      <td style="text-align: right">1.80776e+09</td>
      <td style="text-align: right">1.88793e+09</td>
      <td style="text-align: right">5.28423e+09</td>
      <td style="text-align: right">0.300325</td>
      <td style="text-align: right">0.342104</td>
      <td style="text-align: right">0.357276</td>
      <td style="text-align: right">2345860</td>
      <td style="text-align: right">284978</td>
      <td style="text-align: right">9898</td>
      <td style="text-align: right">2640737</td>
      <td style="text-align: right">0.888335</td>
      <td style="text-align: right">0.107916</td>
      <td style="text-align: right">0.0037482</td>
      <td style="text-align: right">53499</td>
      <td style="text-align: right">49091</td>
      <td style="text-align: right">1.08979</td>
      <td style="text-align: right">0.019</td>
      <td style="text-align: right">5226</td>
      <td style="text-align: right">291955</td>
      <td style="text-align: right">0.0179</td>
      <td style="text-align: right">0.022</td>
      <td style="text-align: right">5457125</td>
      <td style="text-align: right">0.7327</td>
      <td style="text-align: right">0.1528</td>
      <td style="text-align: right">2279</td>
      <td style="text-align: right">1700.5</td>
      <td style="text-align: right">18.2</td>
      <td style="text-align: right">0.0214</td>
      <td style="text-align: right">0.006</td>
      <td style="text-align: right">0.915927</td>
      <td style="text-align: right">0.0840733</td>
      <td style="text-align: right">0.0547874</td>
      <td style="text-align: left">2014-05-11 18:38:00</td>
      <td style="text-align: left">2014-05-11 18:39:00</td>
    </tr>
    <tr>
      <td style="text-align: right">2010</td>
      <td style="text-align: right">10</td>
      <td style="text-align: left">Minnesota</td>
      <td style="text-align: left">MN</td>
      <td style="text-align: left">MRO</td>
      <td style="text-align: left">East North Central</td>
      <td style="text-align: right">-1.5</td>
      <td style="text-align: left">cold</td>
      <td style="text-align: left">2010-10-26</td>
      <td style="text-align: left">20:00:00</td>
      <td style="text-align: left">2010-10-28</td>
      <td style="text-align: left">22:00:00</td>
      <td style="text-align: left">severe weather</td>
      <td style="text-align: left">heavy wind</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">3000</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">70000</td>
      <td style="text-align: right">0.1087</td>
      <td style="text-align: right">0.0819</td>
      <td style="text-align: right">0.0607</td>
      <td style="text-align: right">0.0815</td>
      <td style="text-align: right">1.46729e+09</td>
      <td style="text-align: right">1.80168e+09</td>
      <td style="text-align: right">1.9513e+09</td>
      <td style="text-align: right">5.22212e+09</td>
      <td style="text-align: right">0.280977</td>
      <td style="text-align: right">0.34501</td>
      <td style="text-align: right">0.37366</td>
      <td style="text-align: right">2300291</td>
      <td style="text-align: right">276463</td>
      <td style="text-align: right">10150</td>
      <td style="text-align: right">2586905</td>
      <td style="text-align: right">0.889206</td>
      <td style="text-align: right">0.10687</td>
      <td style="text-align: right">0.00392361</td>
      <td style="text-align: right">50447</td>
      <td style="text-align: right">47287</td>
      <td style="text-align: right">1.06683</td>
      <td style="text-align: right">0.027</td>
      <td style="text-align: right">4571</td>
      <td style="text-align: right">267895</td>
      <td style="text-align: right">0.0170627</td>
      <td style="text-align: right">0.021</td>
      <td style="text-align: right">5310903</td>
      <td style="text-align: right">0.7327</td>
      <td style="text-align: right">0.1528</td>
      <td style="text-align: right">2279</td>
      <td style="text-align: right">1700.5</td>
      <td style="text-align: right">18.2</td>
      <td style="text-align: right">0.0214</td>
      <td style="text-align: right">0.006</td>
      <td style="text-align: right">0.915927</td>
      <td style="text-align: right">0.0840733</td>
      <td style="text-align: right">0.0547874</td>
      <td style="text-align: left">2010-10-26 20:00:00</td>
      <td style="text-align: left">2010-10-28 22:00:00</td>
    </tr>
    <tr>
      <td style="text-align: right">2012</td>
      <td style="text-align: right">6</td>
      <td style="text-align: left">Minnesota</td>
      <td style="text-align: left">MN</td>
      <td style="text-align: left">MRO</td>
      <td style="text-align: left">East North Central</td>
      <td style="text-align: right">-0.1</td>
      <td style="text-align: left">normal</td>
      <td style="text-align: left">2012-06-19</td>
      <td style="text-align: left">04:30:00</td>
      <td style="text-align: left">2012-06-20</td>
      <td style="text-align: left">23:00:00</td>
      <td style="text-align: left">severe weather</td>
      <td style="text-align: left">thunderstorm</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">2550</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">68200</td>
      <td style="text-align: right">0.1179</td>
      <td style="text-align: right">0.0925</td>
      <td style="text-align: right">0.0671</td>
      <td style="text-align: right">0.0919</td>
      <td style="text-align: right">1.85152e+09</td>
      <td style="text-align: right">1.94117e+09</td>
      <td style="text-align: right">1.99303e+09</td>
      <td style="text-align: right">5.78706e+09</td>
      <td style="text-align: right">0.319941</td>
      <td style="text-align: right">0.335433</td>
      <td style="text-align: right">0.344393</td>
      <td style="text-align: right">2317336</td>
      <td style="text-align: right">278466</td>
      <td style="text-align: right">11010</td>
      <td style="text-align: right">2606813</td>
      <td style="text-align: right">0.888954</td>
      <td style="text-align: right">0.106822</td>
      <td style="text-align: right">0.00422355</td>
      <td style="text-align: right">51598</td>
      <td style="text-align: right">48156</td>
      <td style="text-align: right">1.07148</td>
      <td style="text-align: right">0.006</td>
      <td style="text-align: right">5364</td>
      <td style="text-align: right">277627</td>
      <td style="text-align: right">0.0193209</td>
      <td style="text-align: right">0.022</td>
      <td style="text-align: right">5380443</td>
      <td style="text-align: right">0.7327</td>
      <td style="text-align: right">0.1528</td>
      <td style="text-align: right">2279</td>
      <td style="text-align: right">1700.5</td>
      <td style="text-align: right">18.2</td>
      <td style="text-align: right">0.0214</td>
      <td style="text-align: right">0.006</td>
      <td style="text-align: right">0.915927</td>
      <td style="text-align: right">0.0840733</td>
      <td style="text-align: right">0.0547874</td>
      <td style="text-align: left">2012-06-19 04:30:00</td>
      <td style="text-align: left">2012-06-20 23:00:00</td>
    </tr>
    <tr>
      <td style="text-align: right">2015</td>
      <td style="text-align: right">7</td>
      <td style="text-align: left">Minnesota</td>
      <td style="text-align: left">MN</td>
      <td style="text-align: left">MRO</td>
      <td style="text-align: left">East North Central</td>
      <td style="text-align: right">1.2</td>
      <td style="text-align: left">warm</td>
      <td style="text-align: left">2015-07-18</td>
      <td style="text-align: left">02:00:00</td>
      <td style="text-align: left">2015-07-19</td>
      <td style="text-align: left">07:00:00</td>
      <td style="text-align: left">severe weather</td>
      <td style="text-align: left">nan</td>
      <td style="text-align: right">nan</td>
      <td style="text-align: right">1740</td>
      <td style="text-align: right">250000</td>
      <td style="text-align: right">250000</td>
      <td style="text-align: right">0.1307</td>
      <td style="text-align: right">0.1016</td>
      <td style="text-align: right">0.0774</td>
      <td style="text-align: right">0.1043</td>
      <td style="text-align: right">2.02888e+09</td>
      <td style="text-align: right">2.16161e+09</td>
      <td style="text-align: right">1.77794e+09</td>
      <td style="text-align: right">5.97034e+09</td>
      <td style="text-align: right">0.339826</td>
      <td style="text-align: right">0.362059</td>
      <td style="text-align: right">0.297795</td>
      <td style="text-align: right">2374674</td>
      <td style="text-align: right">289044</td>
      <td style="text-align: right">9812</td>
      <td style="text-align: right">2673531</td>
      <td style="text-align: right">0.888216</td>
      <td style="text-align: right">0.108113</td>
      <td style="text-align: right">0.00367005</td>
      <td style="text-align: right">54431</td>
      <td style="text-align: right">49844</td>
      <td style="text-align: right">1.09203</td>
      <td style="text-align: right">0.017</td>
      <td style="text-align: right">4873</td>
      <td style="text-align: right">292023</td>
      <td style="text-align: right">0.016687</td>
      <td style="text-align: right">0.022</td>
      <td style="text-align: right">5489594</td>
      <td style="text-align: right">0.7327</td>
      <td style="text-align: right">0.1528</td>
      <td style="text-align: right">2279</td>
      <td style="text-align: right">1700.5</td>
      <td style="text-align: right">18.2</td>
      <td style="text-align: right">0.0214</td>
      <td style="text-align: right">0.006</td>
      <td style="text-align: right">0.915927</td>
      <td style="text-align: right">0.0840733</td>
      <td style="text-align: right">0.0547874</td>
      <td style="text-align: left">2015-07-18 02:00:00</td>
      <td style="text-align: left">2015-07-19 07:00:00</td>
    </tr>
  </tbody>
</table>

<h4 id="exploratory-data-analysis">Exploratory Data Analysis</h4>

<blockquote>
  <p>Distribution of <code class="language-plaintext highlighter-rouge">U.S._STATE</code>:</p>
</blockquote>
<iframe src="assets/state_distr.html" width="800" height="418" frameborder="0"></iframe>

<p>Here we can see the number of power outages by state. Based on this histogram, we can see that California has the highest number of Power Outages with over 200 in total, and Arkansas has the least amount with barely over 0 in total.</p>

<blockquote>
  <p>Distribution of <code class="language-plaintext highlighter-rouge">U.S._STATE</code> on Map:</p>
</blockquote>
<iframe src="assets/choropleth_map.html" width="800" height="600" frameborder="0"></iframe>

<p>This provides us with a visualization on the geographic areas of the power outages. You can see hrer that majority of the midwest has relatively low power outages and as you move more north east, the number of power outages per state tend to increase</p>

<blockquote>
  <p>Distribution of <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code> per <code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY</code></p>
</blockquote>
<iframe src="assets/climate_X_customer.html" width="800" height="418" frameborder="0"></iframe>

<p>Here we can see a box and whisker plot for each unique <code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY</code>: normal, cold, and warm. All three box and whisker plots seem to each have the same or very similiar values for the lower and upper quartile, and the median. This could tell us that the distribution of <code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY</code> by <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code> are roughly the same, meaning that there may not be a meaningful difference between the catetgories</p>

<blockquote>
  <p>Pairplor of <code class="language-plaintext highlighter-rouge">Utility.PERCEN</code> features</p>
</blockquote>
<iframe src="assets/pair_percen.html" width="800" height="418" frameborder="0"></iframe>

<p>This pairplot gives us all the combinations between all the <code class="language-plaintext highlighter-rouge">.PERCEN</code> columns: <code class="language-plaintext highlighter-rouge">RES.PERCEN</code>, <code class="language-plaintext highlighter-rouge">COM.PERCEN</code>, <code class="language-plaintext highlighter-rouge">IND.PERCEN</code>. You can see here that <code class="language-plaintext highlighter-rouge">RES.PERCEN</code> by <code class="language-plaintext highlighter-rouge">IND.PERCEN</code>, <code class="language-plaintext highlighter-rouge">COM.PERCEN</code> by <code class="language-plaintext highlighter-rouge">IND.PERCEN</code> show an overall negative slope indicating that while as one of the featuers increases, the other decreases. Or, in other words, the two features have an inverse relationship.</p>

<blockquote>
  <p>Pairplor of <code class="language-plaintext highlighter-rouge">Utility.SALES</code> features</p>
</blockquote>
<iframe src="assets/pair_sales.html" width="800" height="418" frameborder="0"></iframe>

<p>This pairplor gives us all the combinations between all the <code class="language-plaintext highlighter-rouge">.SALES</code> columns: <code class="language-plaintext highlighter-rouge">RES.SALES</code>, <code class="language-plaintext highlighter-rouge">COM.SALES</code>, <code class="language-plaintext highlighter-rouge">IND.SALES</code>. Here we can see every combination of features shows a strong positive correlation, suggesting that as one feature increases, the other features also increases</p>

<blockquote>
  <p>Month by <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code></p>
</blockquote>
<iframe src="assets/month_customer.html" width="800" height="418" frameborder="0"></iframe>

<p>Here we can see a line plot where the x-axis is <code class="language-plaintext highlighter-rouge">month</code> and the y-axis is <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code>. Here we can see some patterns wehere in month 3, March, there is a drop is <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code>, but as summer approaches the number steadily inrceases and reached a peak in month 8, August. Then the <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code> drops again by November</p>

<h2 id="assessment-of-missingess">Assessment of Missingess</h2>
<h3 id="nmar-analysis">NMAR Analysis</h3>
<blockquote>
  <p>I am testing the missingness in <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFETCED</code> against the feature <code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY</code></p>
</blockquote>

<iframe src="assets/missing_no_depends_bar.html" width="800" height="418" frameborder="0"></iframe>

<p>Here you can see a distribution of <code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY</code> and the proportion for both when <code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY_MISSING</code> is <code class="language-plaintext highlighter-rouge">True</code> and when it’s <code class="language-plaintext highlighter-rouge">False</code>. Based on the graph we can see both distributions are very smiliar. To be sure I concluded a permutation test and chose Total Variation Distance(TVD) to be the test statistic. I chose this test statstic because this is comparing two categorical distributions</p>

<iframe src="assets/missing_no_depends.html" width="800" height="418" frameborder="0"></iframe>

<p>Based on this, I got a p-value of 0.34. So at the 5% significance level, we can say that <code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY</code> and <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code> may not be statistically significantly associated with each other.</p>

<h3 id="missingness-dependency">Missingness Dependency</h3>
<blockquote>
  <p>Here I am testing the same missingness but dependent on <code class="language-plaintext highlighter-rouge">NERC.REGION</code></p>
</blockquote>

<iframe src="assets/missing_depends_bar.html" width="800" height="418" frameborder="0"></iframe>

<p>Here you can see various categories of <code class="language-plaintext highlighter-rouge">NERC.REGION</code> have trivial data where there is a clear difference between <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED_MISSING = False</code> and <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFETCED_MISSING = True</code>. For example we can see this for the <code class="language-plaintext highlighter-rouge">WECC</code> category, where there is a far higher True proportion then False. To confirm this, I used a permutation test similiar to the one previously used, using the same test statistic, TVD.</p>

<iframe src="assets/missing_depends.html" width="800" height="418" frameborder="0"></iframe>

<p>Based on this Emperical Distribution, you can see that observed TVD is far higher than any TVD from the permutation test. This gives a p-value of 0.0. Where, at the 5% significance level, we can state that there lies a significance association between <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code> missingness and <code class="language-plaintext highlighter-rouge">NERC.REGION</code>.</p>

<h2 id="hypothesis-testing">Hypothesis Testing</h2>
<p>We are investigating whether <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code> and <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code> have the same ditribution, or in other words we want to see if there is a significant association between the two features. This could tell us some more insights on the data and a potential reasoning as to what features play an important part in the <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code>. To do this I am going to use a permutation test on the distribution of <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code> and <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code> to see if there is such an association, or is it just by chance.</p>

<p><strong>Null Hypothesis (H<sub>0</sub>):</strong> The <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code> has no association with <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code> <br />
<strong>Alternative Hypothesis (H<sub>1</sub>):</strong> <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code> is associated with <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code> <br />
<strong>Test Statistic:</strong> Since we are comparing two categorical distributions, <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code> and <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code>, we can use total variation distance (TVD) <br />
<strong>Significance Level:</strong> To ensure a good balance between Type I Error and Type II Error, I chose a significance level at 5% (0.05)</p>

<p>To start I created a contingency table with the index being <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code>, and columns being <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code>, and then divide by the sum to be able to convert these “counts” into proportions”</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">count_table</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">pivot_table</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s">'CLIMATE.REGION'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s">'CAUSE.CATEGORY'</span><span class="p">,</span> <span class="n">aggfunc</span><span class="o">=</span><span class="s">'size'</span><span class="p">)</span>
<span class="n">count_table</span> <span class="o">=</span> <span class="n">count_table</span> <span class="o">/</span> <span class="n">count_table</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>

<span class="n">count_table</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left">CLIMATE.REGION</th>
      <th style="text-align: right">equipment failure</th>
      <th style="text-align: right">fuel supply emergency</th>
      <th style="text-align: right">intentional attack</th>
      <th style="text-align: right">islanding</th>
      <th style="text-align: right">public appeal</th>
      <th style="text-align: right">severe weather</th>
      <th style="text-align: right">system operability disruption</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Central</td>
      <td style="text-align: right">0.118644</td>
      <td style="text-align: right">0.0784314</td>
      <td style="text-align: right">0.0909091</td>
      <td style="text-align: right">0.0652174</td>
      <td style="text-align: right">0.0289855</td>
      <td style="text-align: right">0.177866</td>
      <td style="text-align: right">0.0873016</td>
    </tr>
    <tr>
      <td style="text-align: left">East North Central</td>
      <td style="text-align: right">0.0508475</td>
      <td style="text-align: right">0.0980392</td>
      <td style="text-align: right">0.0478469</td>
      <td style="text-align: right">0.0217391</td>
      <td style="text-align: right">0.0289855</td>
      <td style="text-align: right">0.137022</td>
      <td style="text-align: right">0.0238095</td>
    </tr>
    <tr>
      <td style="text-align: left">Northeast</td>
      <td style="text-align: right">0.0847458</td>
      <td style="text-align: right">0.27451</td>
      <td style="text-align: right">0.322967</td>
      <td style="text-align: right">0.0217391</td>
      <td style="text-align: right">0.057971</td>
      <td style="text-align: right">0.231884</td>
      <td style="text-align: right">0.119048</td>
    </tr>
    <tr>
      <td style="text-align: left">Northwest</td>
      <td style="text-align: right">0.0338983</td>
      <td style="text-align: right">0.0196078</td>
      <td style="text-align: right">0.212919</td>
      <td style="text-align: right">0.108696</td>
      <td style="text-align: right">0.0289855</td>
      <td style="text-align: right">0.0382082</td>
      <td style="text-align: right">0.031746</td>
    </tr>
    <tr>
      <td style="text-align: left">South</td>
      <td style="text-align: right">0.169492</td>
      <td style="text-align: right">0.137255</td>
      <td style="text-align: right">0.0669856</td>
      <td style="text-align: right">0.0434783</td>
      <td style="text-align: right">0.608696</td>
      <td style="text-align: right">0.14888</td>
      <td style="text-align: right">0.214286</td>
    </tr>
  </tbody>
</table>

<p>Next, I needed to find my observed statistic. To do this, I used the pandas.DataFrame.diff() method to be able to find the observed TVD.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">obs_tvd</span> <span class="o">=</span> <span class="n">count_table</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">abs</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span><span class="mi">2</span>
<span class="n">obs_tvd</span> <span class="o">=</span> <span class="n">obs_tvd</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div></div>
<p>I got an obsvered TVD of 2.74. Next, I performed a permutation test of 1000 repetitions. For each permutation test, I added the found TVD into a list where I can later visualize the Emperical Distribution of TVD.</p>

<iframe src="assets/hypothesis_test.html" width="800" height="418" frameborder="0"></iframe>

<p>Based on the disttribution you can see that the observed TVD is a lot higher than the expected TVD. So, when I calculated to find the p-value I got 0.0. This tells me that, at the 5% significance level, we can reject the null hypothesis, where we states that <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code> has no association with <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code>, and we may conclude that there actually may be an association</p>
<h2 id="framing-a-prediction-problem">Framing a Prediction Problem</h2>

<p>To be able to develop and understand a deeper view of the power outages cause, I am creating a predictive model based around the question of “Predict the causes of the power outages”.</p>

<p>This problem would be a classifier since it is trying to predict certain pre-defined categories of power outages cause, instead of a numerical value. Since <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code> is not made up of just two categories, but a variety of them, the typeo fo classifier I am building is a multiclass classification. The response variable will be <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code> since we are trying to predict what caused the power outage, and <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code> is defined by exactly that. To evaluate this classifier, I will be using accuracy since it is a simple straightforward approach that is also easily interpretable. Also, with accuracy you are able to see the proportion of scores that are correct which is what mainly matters. Compared to other metrics such as precision and recall, these are good when we care for either the “False Negative” or “False Positive” more than the other. However, in this case the importance of “False Negative” and “False Positive” are equally the same and there for Precision, Recall, and F1 Score may not be the best evaluation metric to use.</p>

<h2 id="baseline-model">Baseline Model</h2>

<p>For my baseline model, I decided to go with a Decision Tree, using sklearn’s <code class="language-plaintext highlighter-rouge">DecisionTreeClassifier()</code>. I chose this model because I needed something that can identify categories or “classes”, while also being easily manipulable, through its hyperparameters, to best accomadate the power outages dataframe. For this model I decided to drop all the features that contain missing (NaN) values or that were not important to determining the cause. These features are: <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY.DETAIL</code>, <code class="language-plaintext highlighter-rouge">OUTAGE.START.DATE</code>, <code class="language-plaintext highlighter-rouge">OUTAGE.START.TIME</code>, <code class="language-plaintext highlighter-rouge">OUTAGE.RESTORATION.DATE</code>, <code class="language-plaintext highlighter-rouge">OUTAGE.RESTORATION.TIME</code>, <code class="language-plaintext highlighter-rouge">OUTAGE.START</code>, <code class="language-plaintext highlighter-rouge">OUTAGE.RESTORATION</code>, <code class="language-plaintext highlighter-rouge">HURRICANE.NAMES</code>, <code class="language-plaintext highlighter-rouge">DEMAND.LOSS.MW</code>, <code class="language-plaintext highlighter-rouge">OUTAGE.DURATION</code>. After doing so here are the stats for the features I used:</p>

<blockquote>
  <p><strong>Nominal</strong>: (5) <code class="language-plaintext highlighter-rouge">U.S._STATE</code>, <code class="language-plaintext highlighter-rouge">POSTAL.CODOE</code>, <code class="language-plaintext highlighter-rouge">NERC.REGION</code>, <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code>, <code class="language-plaintext highlighter-rouge">CLIMATE.CATEGORY</code> <br />
<strong>Ordinal</strong>: (0) <br />
<strong>Quantitative</strong>: (41) <code class="language-plaintext highlighter-rouge">YEAR</code>, <code class="language-plaintext highlighter-rouge">MONTH</code>, <code class="language-plaintext highlighter-rouge">ANOMALY.LEVEL</code>, <code class="language-plaintext highlighter-rouge">CUSTOMERS.AFFECTED</code>, <code class="language-plaintext highlighter-rouge">RES.PRICE</code>, <code class="language-plaintext highlighter-rouge">COM.PRICE</code>, <code class="language-plaintext highlighter-rouge">IND.PRICE</code>, <code class="language-plaintext highlighter-rouge">TOTAL.PRICE</code>, <code class="language-plaintext highlighter-rouge">RES.SALES</code>, <code class="language-plaintext highlighter-rouge">COM.SALES</code>, <code class="language-plaintext highlighter-rouge">IND.SALES</code>, <code class="language-plaintext highlighter-rouge">TOTAL.SALES</code>, <code class="language-plaintext highlighter-rouge">RES.PERCEN</code>, <code class="language-plaintext highlighter-rouge">COM.PERCEN</code>, <code class="language-plaintext highlighter-rouge">IND.PERCEN</code>, <code class="language-plaintext highlighter-rouge">RES.CUSTOMERS</code>, <code class="language-plaintext highlighter-rouge">COM.CUSTOMERS</code>, <code class="language-plaintext highlighter-rouge">IND.CUSTOMERS</code>, <code class="language-plaintext highlighter-rouge">TOTAL.CUSTOMERS</code>, <code class="language-plaintext highlighter-rouge">RES.CUST.PCT</code>, <code class="language-plaintext highlighter-rouge">COM.CUST.PCT</code>, <code class="language-plaintext highlighter-rouge">IND.CUST.PCT</code>, <code class="language-plaintext highlighter-rouge">PC.REALGSP.STATE</code>, <code class="language-plaintext highlighter-rouge">PC.REALGSP.USA</code>, <code class="language-plaintext highlighter-rouge">PC.REALGSP.REL</code>, <code class="language-plaintext highlighter-rouge">PC.REALGSP.CHANGE</code>, <code class="language-plaintext highlighter-rouge">UTIL.REALGSP</code>, <code class="language-plaintext highlighter-rouge">TOTAL.REALGSP</code>, <code class="language-plaintext highlighter-rouge">UTIL.CONTRI</code>, <code class="language-plaintext highlighter-rouge">PI.UTIL.OFUSA</code>, <code class="language-plaintext highlighter-rouge">POPULATION</code>, <code class="language-plaintext highlighter-rouge">POPPCT_URBAN</code>, <code class="language-plaintext highlighter-rouge">POPPCT_UC</code>, <code class="language-plaintext highlighter-rouge">POPDEN_URBAN</code>, <code class="language-plaintext highlighter-rouge">POPDEN_UC</code>, <code class="language-plaintext highlighter-rouge">POPDEN_RURAL</code>, <code class="language-plaintext highlighter-rouge">AREAPCT_URBAN</code>, <code class="language-plaintext highlighter-rouge">AREAPCT_UC</code>, <code class="language-plaintext highlighter-rouge">PCT_LAND</code>, <code class="language-plaintext highlighter-rouge">PCT_WATER_TOT</code>, <code class="language-plaintext highlighter-rouge">PCT_WATER_INLAND</code></p>
</blockquote>

<p>To determine the “importance” of each feature, I decided to perform an hypothesis test, where I tested the association between the feature and <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code>, after obtaining all the p-values I decided to pick the features that have extremeley low p-values or show significance at the 5% level. After filtering out the dataframe to only include these features and excluding the <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code> column, I got 46 features to include in my model. Out of these 46, the categorical columns were the 5 nominal features stated above, To handle these categorical columns so that it can be used in our decision tree I decided to One Hot Encode these features using sci-kit learn’s <code class="language-plaintext highlighter-rouge">OneHotEncoder</code>, which I did inside a <code class="language-plaintext highlighter-rouge">ColumnTransformer</code> method.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categorical_cols</span> <span class="o">=</span> <span class="n">decision_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'CAUSE.CATEGORY'</span><span class="p">]).</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'object'</span><span class="p">]).</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s">'onehot'</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="s">'first'</span><span class="p">,</span> <span class="n">handle_unknown</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">),</span> <span class="n">categorical_cols</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">remainder</span><span class="o">=</span><span class="s">'passthrough'</span><span class="p">)</span>
</code></pre></div></div>
<p>After defining the preprocessing step, I added <code class="language-plaintext highlighter-rouge">preprocessor</code> into a pipeline contianing this <code class="language-plaintext highlighter-rouge">preprocessor</code> and the <code class="language-plaintext highlighter-rouge">DecisionTreeClassifier()</code>. This makes using future process much easier since everything would all be in one place and you can run <code class="language-plaintext highlighter-rouge">.fit()</code> once instead of individually running it for each method.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'preproc'</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'classifier'</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span><span class="p">())</span>
<span class="p">])</span>
</code></pre></div></div>

<p>For the decision tree I used, there are many hyparameters that I can use to make it the most effective as possible. To achieve the best hyperparameter combination, I used a grid search from sci-kit learn’s <code class="language-plaintext highlighter-rouge">GridSearchCV</code>. In my <code class="language-plaintext highlighter-rouge">param_grid</code> dictionary, I included the hyperameters of <code class="language-plaintext highlighter-rouge">criterion</code>, <code class="language-plaintext highlighter-rouge">max_depth</code>, and <code class="language-plaintext highlighter-rouge">min_samples_split</code>. After fitting the grid search, we find that the best parameters comes out to be:</p>

<blockquote>
  <p><strong>criterion</strong>: “gini” <br />
<strong>max_depth</strong>: 2 <br />
<strong>min_samples_split</strong>: 2</p>
</blockquote>

<p>With using these hyperparameter values, we obtain an accuracy of:</p>
<blockquote>
  <p><strong>Train Score:</strong> 65.45% <br />
<strong>Train Score:</strong> 63.3%</p>
</blockquote>

<h2 id="final-model">Final Model</h2>

<p>Based on the baseline model, we see here that both the train and test score accuracy are lower percentages. To try and increase the score, I decided to perform binning transformation on all the features that explain the utility demographics such as the ones that are <code class="language-plaintext highlighter-rouge">___.SALES</code>, <code class="language-plaintext highlighter-rouge">___.PRICE</code>, <code class="language-plaintext highlighter-rouge">___.PERCEN</code>, <code class="language-plaintext highlighter-rouge">___.CUSTOMERS</code>. I chose these values because they have large ranges within each feature, and so it may be better to group them by their value. This would make the decision tree make “decisions” easier since it is then based on groups which might generalize the tree much better. With a more generalized decision tree, it would might decrease the training score accuracy, but it would increase the test score accuracy, which is more important for us.</p>

<p>I also chose to perform a polynomial transformation of cerrtain features. The features I chose for this were: <code class="language-plaintext highlighter-rouge">PC.REALGSP.STATE</code>, <code class="language-plaintext highlighter-rouge">PC.REALGSP.USA</code>, <code class="language-plaintext highlighter-rouge">PC.REALGSP.REL</code>, <code class="language-plaintext highlighter-rouge">PC.REALGSP.CHANGE</code>, <code class="language-plaintext highlighter-rouge">UTIL.REALGSP</code>, <code class="language-plaintext highlighter-rouge">TOTAL.REALGSP</code>, <code class="language-plaintext highlighter-rouge">UTIL.CONTRI</code>, <code class="language-plaintext highlighter-rouge">PI.UTIL.OFUSA</code>, <code class="language-plaintext highlighter-rouge">POPPCT_URBAN</code>, <code class="language-plaintext highlighter-rouge">POPPCT_UC</code>, <code class="language-plaintext highlighter-rouge">POPDEN_URBAN</code>, <code class="language-plaintext highlighter-rouge">POPDEN_UC</code>, <code class="language-plaintext highlighter-rouge">POPDEN_RURAL</code>, <code class="language-plaintext highlighter-rouge">AREAPCT_URBAN</code>, <code class="language-plaintext highlighter-rouge">AREAPCT_UC</code>. These features all have a non-linear relationship, so by adding a polynomial transformation, it can help captures its relationship. It also allows the model to capture the interaction effects between different features, which might give an insight for the <code class="language-plaintext highlighter-rouge">CAUSE.CATEGORY</code>. By including this feature into our decision tree, it may improve the decision boundaries, which can make the class distinction more clearer and better fit the data. At first I tried running a <code class="language-plaintext highlighter-rouge">DecisionTreeClassifier()</code>, however the training accuracy only increased slightly. So I tried using a random forest.</p>

<p>I tried using a random forest classifier through sci-kit learn’s <code class="language-plaintext highlighter-rouge">RandomForestClassifier()</code>. I chose this classifier because it solves the problem of always getting the same decision tree when using the same training data. When using a RandomForest you can use a subset of features through randomnessm which solves the problem and creates a more generalized tree that may work for more instances not in the training data. For my Random Forest, I used the same idea of a grid search to find its best hyperparameters. In my <code class="language-plaintext highlighter-rouge">param_grid</code> I included: <code class="language-plaintext highlighter-rouge">n_estimators</code>, <code class="language-plaintext highlighter-rouge">max_depth</code>, <code class="language-plaintext highlighter-rouge">min_samples_split</code>. I also chose a k-fold of 5, which should give enough folds to test all combinations. After running all of these hyperparameterscombinations, the highest performing hyperparameters are:</p>
<blockquote>
  <p><strong>max_depth:</strong> 14 <br />
<strong>min_samples_split:</strong> 7 <br />
<strong>n_estimators:</strong> 103</p>
</blockquote>

<p>with this hyperparameter combinations: I found an accuracy of:</p>
<blockquote>
  <p><strong>Train Score:</strong> 88.37%
<strong>Test Score:</strong> 69.15%</p>
</blockquote>

<p>As you can see, compared to the baseline model, we see an overall improvement in the training and test score. We see a higher jump in accuracy for training, which makes sense because with our new features we added, we can capture more relationships between features. Also, wiht a random forest classifier, it adjusts for generalization which also leads to a higher training accuracy. Due to the features we added and the nature of Random Forest Classifier’s it’s clear how the test accuracy naturally increases as well.</p>

<h2 id="fairness-analysis">Fairness Analysis</h2>

<p>For my Fairness Analysis, I chose to compare the two groups of region, based on <code class="language-plaintext highlighter-rouge">CLIAMTE.REGION</code>. To do this, I binarized <code class="language-plaintext highlighter-rouge">CLIMATE.REGION</code> to create two regions, East and West.</p>
<blockquote>
  <p><strong>Group 1:</strong> Northeast, South, Southeast, East North Central (represented as ‘East’)<br />
<strong>Group 2:</strong> West, Central, Northwest, Southwestm West North Central (represented as West) \</p>
</blockquote>

<p><strong>Evaluation Metric:</strong> The evaluation metric I used was the <strong>f1_score</strong>. I used the f1 score because our distribution of the two categories are not balanced, so by using the f1-score it takes into account the unbalanced nature.</p>

<p>The <strong>Hypothesis Test</strong> I used was:</p>

<p><strong>H<sub>0</sub>:</strong> Our model is fair. Its F1 score for West and East US are roughly the same, and any differences are due to random chance <br />
<strong>H<sub>1</sub>:</strong> Our model is unfair. Its F1 score for West is lower than the East</p>

<p><strong>Test Statistic:</strong> The Test statistic I used is the difference in F1 score. To do this I used <code class="language-plaintext highlighter-rouge">pandas</code> <code class="language-plaintext highlighter-rouge">.diff()</code> method between the two groups of East and West.</p>

<p>After finding the observed test statstic, I ran a permutation test with 1000 iterations and added all the test statistics into a list <code class="language-plaintext highlighter-rouge">diff_in_f1</code>. When taking all of these values and plotting the distribution, I got this:</p>

<iframe src="assets/fairness_eval.html" width="800" height="418" frameborder="0"></iframe>

<p><strong>P-Value:</strong> As you can see on the visualization, using our p-value definition of:</p>

<p><code class="language-plaintext highlighter-rouge">py (np.array(diff_in_f1) &lt; obs_diff).mean()</code></p>

<p>Based on this we get a p-value of 0.00. This tells us that we can reject the null hypothesis and say that there is evidence to conclude that the F1 score for West may be significantly lower than the F1 socre for East.</p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/vip11017/power_outage_analysis">power_outage_analysis</a> is maintained by <a href="https://github.com/vip11017">vip11017</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
